# excel_etl/runner.py

from __future__ import annotations

import re
import json
from pathlib import Path
from typing import Callable, Optional, List, Tuple, Dict, Any

import yaml

from .utils.logger import get_logger
from .utils.resources import resource_path
from .utils.io import write_to_template
from .extractors.registry import get_extractor

# IMPORTANT: ensure extractors import/registration happens
# (excel_etl/extractors/__init__.py should import all extractor modules)
from . import extractors as _ensure_registration  # noqa: F401

log = get_logger("runner")


def _load_yaml(relpath: str) -> dict:
    """
    Load a YAML file from the bundled resources (works in dev and PyInstaller).
    """
    return yaml.safe_load(resource_path(relpath).read_text(encoding="utf-8"))


def _choose_extractor(path: Path, routing_cfg: dict, category: Optional[str] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    """
    Decide which extractor to use for a given input file based on routing rules.
    Current rules support filename_regex; extend as needed.
    """
    fname = path.name
    for rule in routing_cfg.get("routes", []):
        cond = rule.get("when", {})
        ok = True

        # in-folder category match (case-insensitive)
        if "in_folder" in cond and category is not None:
            ok &= (cond["in_folder"].lower() == category.lower())

        # filename regex match (case-insensitive)
        if "filename_regex" in cond:
            ok &= bool(re.match(cond["filename_regex"], fname, flags=re.IGNORECASE))

        # filename contains (case-insensitive)
        if "filename_contains" in cond:
            ok &= (cond["filename_contains"].lower() in fname.lower())

        # future: add sheet_contains, etc.

        if ok:
            return rule["use_extractor"], dict(rule.get("options", {}))

    return routing_cfg.get("default_extractor"), {}


def _gather_input_files(in_dir: Path, exts: List[str]) -> List[Path]:
    """
    Collect files in the input directory matching the given list of extensions.
    """
    files: List[Path] = []
    for ext in exts:
        files.extend(in_dir.glob(f"*{ext}"))
    # sort by name for deterministic order
    return sorted(files, key=lambda p: p.name.lower())


def _gather_category_files(master_dir: Path, category: str, exts: List[str]) -> List[Path]:
    """
    Collect files from a specific subfolder (category) under the master input folder.
    """
    cat_dir = master_dir / category
    if not cat_dir.is_dir():
        return []
    return _gather_input_files(cat_dir, exts)


def run_batch(
    input_dir: str,
    out_dir: str,
    template_path: str,
    log_cb: Optional[Callable[[str], None]] = None,
) -> None:
    """
    Process all supported files in input_dir, route each to an extractor,
    write results into the provided Excel template, and save outputs to out_dir.

    Loads bundled configs:
      - config/routing.yaml
      - config/template_map.yaml

    Args:
        input_dir: folder containing raw input files (csv/xls/xlsx/xlsb)
        out_dir: folder to write populated outputs
        template_path: path to the output template .xlsx chosen by the user
        log_cb: optional callback to emit log lines (used by the Tkinter UI)
    """

    def emit(msg: str) -> None:
        if log_cb:
            log_cb(msg)
        else:
            log.info(msg.rstrip("\n"))

    # Load bundled configs
    routing_cfg = _load_yaml("config/routing.yaml")
    template_map = _load_yaml("config/template_map.yaml")

    in_dir = Path(input_dir)
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)

    categories = ["frp_rwa", "frp_pbt", "growth_rate", "discount_rate"]

    all_files: List[Path] = []
    cat_to_files: dict = {}
    for cat in categories:
        files = _gather_category_files(in_dir, cat, exts=[".csv", ".xls", ".xlsx", ".xlsb"])
        cat_to_files[cat] = files
        all_files.extend(files)

    emit(f"Found {len(all_files)} input files across categories.\n")

    summary = []
    processed_count = 0

    for cat in categories:
        files = cat_to_files.get(cat, [])
        if not files:
            emit(f"[{cat}] No files found.\n")
            continue

        emit(f"[{cat}] {len(files)} file(s)\n")

        cat_out_dir = out / cat
        cat_out_dir.mkdir(parents=True, exist_ok=True)

        for i, fp in enumerate(files, 1):
            emit(f"  [{i}/{len(files)}] {fp.name}\n")
            try:
                # Pick extractor by routing rules
                ex_name, ex_opts = _choose_extractor(fp, routing_cfg, category=cat)
                if not ex_name:
                    emit("    No extractor matched. Skipping.\n")
                    summary.append({"file": fp.name, "category": cat, "status": "skipped"})
                    continue

                extractor_cls = get_extractor(ex_name)
                extractor = extractor_cls()
                emit(f"    Using extractor: {ex_name}\n")

                # IMPORTANT: pass the file path; extractor reads via readers.read_any()
                result = extractor.extract(fp, options=ex_opts)

                # Write to the template with category-specific mapping
                out_file = cat_out_dir / f"{fp.stem}_out.xlsx"
                write_to_template(template_path, template_map, result, out_file, category=cat)
                emit(f"    Wrote: {out_file.relative_to(out)}\n")

                summary.append({"file": fp.name, "category": cat, "status": "ok", **result})
                processed_count += 1
            except Exception as e:
                emit(f"    ERROR: {e}\n")
                summary.append({"file": fp.name, "category": cat, "status": "error", "error": str(e)})

    # Save a machine-readable run summary
    (out / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
    emit("Done.\n")
