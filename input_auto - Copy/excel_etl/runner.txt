# excel_etl/runner.py

from __future__ import annotations

import re
import json
from pathlib import Path
from typing import Callable, Optional, List, Tuple, Dict, Any
import pandas as pd

import yaml

from .utils.logger import get_logger
from .utils.resources import resource_path
# from .utils.io import write_to_template  # disabled: no xlsx output creation per current requirements
from .extractors.registry import get_extractor

# IMPORTANT: ensure extractors import/registration happens
# (excel_etl/extractors/__init__.py should import all extractor modules)
from . import extractors as _ensure_registration  # noqa: F401

log = get_logger("runner")


def _load_yaml(relpath: str) -> dict:
    """
    Load a YAML file from the bundled resources (works in dev and PyInstaller).
    """
    return yaml.safe_load(resource_path(relpath).read_text(encoding="utf-8"))


def _choose_extractor(path: Path, routing_cfg: dict, category: Optional[str] = None) -> Tuple[Optional[str], Dict[str, Any]]:
    """
    Decide which extractor to use for a given input file based on routing rules.
    Current rules support filename_regex; extend as needed.
    """
    fname = path.name
    for rule in routing_cfg.get("routes", []):
        cond = rule.get("when", {})
        ok = True

        # in-folder category match (case-insensitive)
        if "in_folder" in cond and category is not None:
            ok &= (cond["in_folder"].lower() == category.lower())

        # filename regex match (case-insensitive)
        if "filename_regex" in cond:
            ok &= bool(re.match(cond["filename_regex"], fname, flags=re.IGNORECASE))

        # filename contains (case-insensitive)
        if "filename_contains" in cond:
            ok &= (cond["filename_contains"].lower() in fname.lower())

        # future: add sheet_contains, etc.

        if ok:
            return rule["use_extractor"], dict(rule.get("options", {}))

    return routing_cfg.get("default_extractor"), {}


def _gather_input_files(in_dir: Path, exts: List[str]) -> List[Path]:
    """
    Collect files in the input directory matching the given list of extensions.
    """
    files: List[Path] = []
    for ext in exts:
        files.extend(in_dir.glob(f"*{ext}"))
    # sort by name for deterministic order
    return sorted(files, key=lambda p: p.name.lower())


def _gather_category_files(master_dir: Path, category: str, exts: List[str]) -> List[Path]:
    """
    Collect files from a specific subfolder (category) under the master input folder.
    """
    cat_dir = master_dir / category
    if not cat_dir.is_dir():
        return []
    return _gather_input_files(cat_dir, exts)


def run_batch(
    input_dir: str,
    out_dir: str,
    template_path: str,
    log_cb: Optional[Callable[[str], None]] = None,
) -> None:
    """
    Process inputs under category subfolders and build a master output dictionary.
    Do NOT create any Excel output; only aggregate values for downstream use.

    Loads bundled configs:
      - config/routing.yaml
      - config/template_map.yaml

    Args:
        input_dir: folder containing raw input files (csv/xls/xlsx/xlsb)
        out_dir: folder to write populated outputs
        template_path: path to the output template .xlsx chosen by the user
        log_cb: optional callback to emit log lines (used by the Tkinter UI)
    """

    def emit(msg: str) -> None:
        if log_cb:
            log_cb(msg)
        else:
            log.info(msg.rstrip("\n"))

    # Load bundled configs
    routing_cfg = _load_yaml("config/routing.yaml")
    # template_map is not needed when not writing workbooks

    in_dir = Path(input_dir)
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)

    categories = ["frp_rwa", "frp_pbt", "growth_rate", "discount_rate"]

    all_files: List[Path] = []
    cat_to_files: dict = {}
    for cat in categories:
        files = _gather_category_files(in_dir, cat, exts=[".csv", ".xls", ".xlsx", ".xlsb"])
        cat_to_files[cat] = files
        all_files.extend(files)

    emit(f"Found {len(all_files)} input files across categories.\n")

    # Load user mapping from user_input.xlsx (if present)
    mapper_path = in_dir / "user_input.xlsx"
    mapping: Dict[Tuple[str, str], str] = {}
    if mapper_path.is_file():
        try:
            df_map = pd.read_excel(mapper_path, sheet_name="rwa_pbt_mapper")
            # expected columns: mapping (frp_rwa/frp_pbt), year (Opening/2025..), filename keyword (lowercase)
            for _, row in df_map.iterrows():
                mval = str(row.get("mapping", "")).strip()
                year = str(row.get("year", "")).strip()
                kw = str(row.get("filename keyword", "")).strip().lower()
                if mval and year and kw:
                    mapping[(mval, year)] = kw
            emit("Loaded user mapping from user_input.xlsx.\n")
        except Exception as e:
            emit(f"WARNING: Could not read user_input.xlsx mapping: {e}\n")
    else:
        emit("INFO: user_input.xlsx not found; proceeding without keyword mapping.\n")

    summary = []
    processed_count = 0

    # Initialize master output structure
    years = ["Opening", "2025", "2026", "2027", "2028", "2029"]
    output: Dict[str, Dict[str, Any]] = {
        "frp_rwa": {"Opening": {"Total": None, "CIB": None, "IWPB": None, "CC": None}, **{y: None for y in years if y != "Opening"}},
        "frp_pbt": {y: None for y in years},
        "growth_rate": {y: None for y in years},
        "discount_rate": {y: None for y in years},
    }

    for cat in categories:
        files = cat_to_files.get(cat, [])
        if not files:
            emit(f"[{cat}] No files found.\n")
            continue

        emit(f"[{cat}] {len(files)} file(s)\n")

        # No per-file Excel output; we still ensure category output folder exists for JSON outputs
        cat_out_dir = out / cat
        cat_out_dir.mkdir(parents=True, exist_ok=True)

        # Category-specific handling
        if cat == "frp_rwa":
            # Determine the 'Opening' source file based on mapping keyword; fallback to first file
            opening_kw = mapping.get(("frp_rwa", "Opening"))
            chosen_file: Optional[Path] = None
            if opening_kw:
                for fp in files:
                    if opening_kw in fp.name.lower():
                        chosen_file = fp
                        break
            if chosen_file is None and files:
                chosen_file = files[0]

            if chosen_file is not None:
                emit(f"  [frp_rwa:Opening] Using file {chosen_file.name}\n")
                try:
                    extractor_cls = get_extractor("rwa_opening")
                    extractor = extractor_cls()
                    sheet_to_key = {"Consolidated": "Total", "CIB": "CIB", "IWB": "IWPB", "OTH": "CC"}
                    for sheet_name, subkey in sheet_to_key.items():
                        try:
                            res = extractor.extract(chosen_file, options={"sheet": sheet_name})
                            val = res.get("value") if isinstance(res, dict) else None
                            output["frp_rwa"]["Opening"][subkey] = val
                            emit(f"    {sheet_name} -> {subkey}: {val}\n")
                        except Exception as e:
                            output["frp_rwa"]["Opening"][subkey] = None
                            emit(f"    WARN: {sheet_name} extraction failed: {e}\n")
                except Exception as e:
                    emit(f"    ERROR: rwa_opening failed on {chosen_file.name}: {e}\n")
            else:
                emit("  [frp_rwa:Opening] No suitable file found.\n")
        else:
            # Other categories: leave values as None for now
            pass

    # Save outputs
    (out / "summary.json").write_text(json.dumps(summary, indent=2), encoding="utf-8")
    (out / "output_data.json").write_text(json.dumps(output, indent=2), encoding="utf-8")
    emit("Aggregation complete.\n")
